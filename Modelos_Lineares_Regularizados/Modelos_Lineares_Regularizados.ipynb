{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c26054b-7683-4ee5-b646-f4dbcca08044",
   "metadata": {},
   "source": [
    "# Modelos Lineares Regularizados\n",
    "\n",
    "## üí° O que s√£o\n",
    "\n",
    "Modelos **lineares regularizados** s√£o modelos lineares comuns (como Regress√£o Linear ou Regress√£o Log√≠stica) que recebem um ajuste extra chamado **regulariza√ß√£o**.\n",
    "\n",
    "Esse ajuste serve para evitar que o modelo **decore demais os dados** ‚Äî ou seja, para evitar **overfitting**.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Para que servem\n",
    "\n",
    "Eles servem para:\n",
    "\n",
    "- **Melhorar a generaliza√ß√£o** ‚Üí o modelo aprende o padr√£o dos dados sem se prender aos ru√≠dos.  \n",
    "- **Evitar overfitting** ‚Üí o modelo n√£o fica ‚Äúviciado‚Äù nos dados de treino.  \n",
    "- **Controlar a complexidade** ‚Üí quanto mais forte for a regulariza√ß√£o, mais simples ser√° o modelo.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Como funcionam\n",
    "\n",
    "Vamos pensar na **Regress√£o Linear** simples:\n",
    "\n",
    "$$\n",
    "y = w_1x_1 + w_2x_2 + ... + w_nx_n + b\n",
    "$$\n",
    "\n",
    "O modelo aprende os pesos (`w‚ÇÅ, w‚ÇÇ, ...`) para prever o valor de `y`.\n",
    "\n",
    "üëâ **Sem regulariza√ß√£o:** o modelo tenta ajustar perfeitamente os dados de treino ‚Äî mesmo que isso cause *overfitting*.  \n",
    "üëâ **Com regulariza√ß√£o:** o modelo tenta ajustar os dados, mas **penaliza pesos muito grandes**, for√ßando o modelo a ser mais equilibrado.\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ Tipos de regulariza√ß√£o\n",
    "\n",
    "### 1. Ridge (L2)\n",
    "\n",
    "- Penaliza o **quadrado dos pesos**.  \n",
    "- Tende a deixar todos os pesos pequenos, mas **n√£o zera nenhum**.\n",
    "\n",
    "**F√≥rmula da penaliza√ß√£o:**\n",
    "\n",
    "$$\n",
    "Penalidade = \\lambda \\sum w_i^2\n",
    "$$\n",
    "\n",
    "**Implementa√ß√£o no Scikit-Learn:**\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Ridge\n",
    "modelo = Ridge(alpha=1.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7fd082-b8cf-4052-be57-e4e76e3b322c",
   "metadata": {},
   "source": [
    "### 2. Lasso (L1)\n",
    "\n",
    "Penaliza o **valor absoluto dos pesos**.  \n",
    "Faz alguns pesos virarem **zero**, o que ajuda na **sele√ß√£o de vari√°veis**.\n",
    "\n",
    "**F√≥rmula da penaliza√ß√£o:**\n",
    "\n",
    "$$\n",
    "Penalidade = \\lambda \\sum |w_i|\n",
    "$$\n",
    "\n",
    "**Implementa√ß√£o:**\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Lasso\n",
    "modelo = Lasso(alpha=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b871e127-c050-4ab8-89ad-5c92a87287e9",
   "metadata": {},
   "source": [
    "### 3. Elastic Net\n",
    "\n",
    "Combina **L1 e L2** ‚Äî √© um **meio-termo entre Ridge e Lasso**.\n",
    "\n",
    "**Implementa√ß√£o:**\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import ElasticNet\n",
    "modelo = ElasticNet(alpha=0.1, l1_ratio=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4fa28f-fb71-4ef7-a5d4-ae92381a8d4d",
   "metadata": {},
   "source": [
    "üìä **Resumo simples**\n",
    "\n",
    "| Tipo        | Penaliza√ß√£o          | Efeito Principal                          |\n",
    "|------------|--------------------|------------------------------------------|\n",
    "| Ridge (L2)  | Quadrado dos pesos  | Reduz todos os pesos, mas n√£o zera       |\n",
    "| Lasso (L1)  | Valor absoluto dos pesos | Zera alguns pesos (seleciona features) |\n",
    "| Elastic Net | Mistura L1 e L2     | Equil√≠brio entre os dois                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccef4f22-cbfd-4477-9a38-1cdf78f59aa0",
   "metadata": {},
   "source": [
    "# **Modelos** \n",
    "\n",
    "## **Parada Antecipada (Early Stopping)**\n",
    "\n",
    "üß© **O que √© Parada Antecipada (Early Stopping)**\n",
    "\n",
    "√â uma t√©cnica que interrompe o treinamento antes que o modelo comece a decorar os dados (overfitting).\n",
    "\n",
    "üëâ **Ou seja:**  \n",
    "O modelo vai aprendendo com os dados de treino, mas a cada √©poca (ou itera√ß√£o) ele √© testado nos dados de valida√ß√£o.  \n",
    "Quando o erro de valida√ß√£o para de diminuir e come√ßa a aumentar, o treinamento √© interrompido automaticamente.\n",
    "\n",
    "üéØ **Para que serve**\n",
    "\n",
    "- Evitar overfitting (parar no momento certo).  \n",
    "- Economizar tempo (n√£o treina mais do que precisa).  \n",
    "- Melhorar a generaliza√ß√£o (modelo fica mais equilibrado entre treino e teste).\n",
    "\n",
    "‚öôÔ∏è **Como funciona na pr√°tica**\n",
    "\n",
    "- O modelo √© treinado por v√°rias √©pocas (itera√ß√µes).  \n",
    "- Ap√≥s cada √©poca, √© medido o erro no conjunto de valida√ß√£o.  \n",
    "- Se o erro de valida√ß√£o piorar por muitas √©pocas seguidas, o treinamento para automaticamente.  \n",
    "- O modelo final √© o melhor antes de come√ßar a piorar.\n",
    "\n",
    "‚öñÔ∏è **Comparando Regulariza√ß√£o e Early Stopping**\n",
    "\n",
    "| T√©cnica                  | Como age                         | O que controla                       |\n",
    "|--------------------------|---------------------------------|-------------------------------------|\n",
    "| Regulariza√ß√£o (L1/L2)    | Penaliza pesos grandes           | Complexidade do modelo               |\n",
    "| Parada antecipada        | Interrompe o treino no ponto ideal | Tempo de aprendizado e overfitting |\n",
    "\n",
    "üëâ Ambas reduzem overfitting, mas de formas diferentes:\n",
    "\n",
    "- **Regulariza√ß√£o** atua diretamente nos pesos.  \n",
    "- **Early stopping** atua no processo de treino.\n",
    "\n",
    "üß† **Resumo simples:**\n",
    "\n",
    "- Modelos lineares regularizados ‚Üí limitam o tamanho dos pesos.  \n",
    "- Parada antecipada ‚Üí limita o tempo de treino.  \n",
    "\n",
    "‚û°Ô∏è Usar os dois juntos √© muito comum para garantir um modelo simples, r√°pido e bem ajustado.\n",
    "\n",
    "üßÆ **Em modelos lineares regularizados**\n",
    "\n",
    "Alguns modelos do Scikit-Learn j√° t√™m early stopping embutido.  \n",
    "Por exemplo:\n",
    "\n",
    "üîπ**Regress√£o Log√≠stica com regulariza√ß√£o + parada antecipada**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a22a4fde-b07a-4187-b32e-59dd432db1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acur√°cia: 0.815\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dados artificiais\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Modelo linear regularizado (penalidade L2) com parada antecipada\n",
    "modelo = SGDClassifier(\n",
    "    loss='log_loss',       # Regress√£o log√≠stica\n",
    "    penalty='l2',          # Regulariza√ß√£o Ridge\n",
    "    alpha=0.001,           # For√ßa da regulariza√ß√£o\n",
    "    early_stopping=True,   # <- ativa a parada antecipada\n",
    "    validation_fraction=0.1,  # usa 10% dos dados de treino para validar\n",
    "    n_iter_no_change=5,    # para se n√£o houver melhora em 5 √©pocas\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "modelo.fit(X_train, y_train)\n",
    "print(\"Acur√°cia:\", modelo.score(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f859e7f-7f2b-48a2-a537-2fa473a1cfef",
   "metadata": {},
   "source": [
    "## **Ridge Regression** \n",
    "\n",
    "üí° **O que √© o Ridge Regression (L2 Regularization)**\n",
    "\n",
    "O **Ridge** √© um modelo linear regularizado que adiciona uma penaliza√ß√£o **L2** sobre os pesos do modelo.\n",
    "\n",
    "üëâ Ele √© basicamente uma **Regress√£o Linear comum**, mas com uma ‚Äúpuni√ß√£o‚Äù para pesos muito grandes.  \n",
    "Isso ajuda o modelo a **n√£o se ajustar demais aos dados de treino** (evita overfitting).\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **Para que serve**\n",
    "\n",
    "O Ridge serve para:\n",
    "\n",
    "- Evitar **overfitting**, for√ßando o modelo a ser mais simples.  \n",
    "- Melhorar a **generaliza√ß√£o**, tornando as previs√µes mais est√°veis.  \n",
    "- Controlar a **influ√™ncia das vari√°veis**, reduzindo o impacto de outliers (valores muito extremos).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è **Como funciona**\n",
    "\n",
    "O Ridge modifica a fun√ß√£o de custo da Regress√£o Linear.  \n",
    "A fun√ß√£o normal √©:\n",
    "\n",
    "$$\n",
    "J(w) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "O Ridge adiciona uma penaliza√ß√£o L2:\n",
    "\n",
    "$$\n",
    "J(w) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{n} w_j^2\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ **Onde:**\n",
    "\n",
    "- \\( w_j \\): pesos do modelo  \n",
    "- \\( \\lambda \\): for√ßa da regulariza√ß√£o (**em Scikit-Learn √© o `alpha`**)  \n",
    "\n",
    "üëâ Se **Œª for grande**, os pesos ficam bem pequenos ‚Üí o modelo √© mais simples.  \n",
    "üëâ Se **Œª for pequeno**, o modelo fica mais parecido com a regress√£o linear normal.\n",
    "\n",
    "---\n",
    "\n",
    "‚öôÔ∏è **Par√¢metro principal**\n",
    "\n",
    "| Par√¢metro | Significado |\n",
    "|------------|-------------|\n",
    "| `alpha` | Controla a for√ßa da regulariza√ß√£o (quanto maior, mais forte a penaliza√ß√£o) |\n",
    "\n",
    "\n",
    "### üßÆ **F√≥rmula intuitiva**\n",
    "\n",
    "> ‚ÄúAjuste bem os dados, mas sem deixar os pesos ficarem exageradamente grandes.‚Äù\n",
    "\n",
    "üß† **Exemplo pr√°tico**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75c6038e-57f4-41ee-9474-de7232fd00f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes: [27.97717734 73.20206936 18.83648347]\n",
      "Intercepto: 1.9957035364264062\n",
      "R¬≤ no teste: 0.9599461087753994\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Gerar dados artificiais\n",
    "X, y = make_regression(n_samples=100, n_features=3, noise=15, random_state=42)\n",
    "\n",
    "# Dividir treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Modelo Ridge (regulariza√ß√£o L2)\n",
    "modelo = Ridge(alpha=1.0)  # alpha controla a for√ßa da regulariza√ß√£o\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Resultados\n",
    "#.coef_ (ou ‚Äúcoeficientes‚Äù) s√£o os pesos aprendidos pelo modelo durante o treinamento.\n",
    "#Cada valor representa a import√¢ncia de uma vari√°vel (feature) na previs√£o.\n",
    "# Em resumo:\n",
    "# .coef_ mostra a import√¢ncia e o sentido (positivo ou negativo) de cada vari√°vel.\n",
    "# No Ridge, esses coeficientes s√£o diminu√≠dos para deixar o modelo mais simples e est√°vel.\n",
    "print(\"Coeficientes:\", modelo.coef_)\n",
    "print(\"Intercepto:\", modelo.intercept_)\n",
    "print(\"R¬≤ no teste:\", modelo.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d831f7d-d9b5-4b83-bc57-71c05d3ee2ea",
   "metadata": {},
   "source": [
    "üß© **Parada antecipada (Early Stopping) com Ridge**\n",
    "\n",
    "O `Ridge` padr√£o (`sklearn.linear_model.Ridge`) **n√£o possui early stopping direto**.  \n",
    "Mas podemos usar o `SGDRegressor`, que implementa o Ridge usando **descida de gradiente** e **permite parada antecipada**.\n",
    "\n",
    "üìâ **Como funciona a parada antecipada no Ridge**\n",
    "\n",
    "1. O modelo come√ßa a treinar usando **descida de gradiente**.  \n",
    "2. A cada √©poca, o **erro no conjunto de valida√ß√£o** √© medido.  \n",
    "3. Se o erro **n√£o melhora ap√≥s v√°rias √©pocas seguidas**, o treino **para automaticamente**.  \n",
    "4. Assim, o modelo **interrompe o treinamento antes de come√ßar a overfittar**.\n",
    "\n",
    "üß† **Exemplo com Early Stopping**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb995286-d87b-4a17-abbb-6b5174409ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes: [ 7.95084958e+01  9.89919255e+01  5.16151057e+00  5.63156195e-01\n",
      "  8.64891391e+01 -9.57395750e-01  6.94473848e+01  1.18981859e-02\n",
      "  2.21711091e-01 -2.58639595e-01  1.93992340e+01  3.99247978e+01\n",
      "  2.33367362e-01  2.95170311e+00 -6.60006380e-01  2.66562963e+01\n",
      " -4.79176338e-01  8.70682536e+01  6.79715163e-01  2.67654076e-01]\n",
      "R¬≤ (valida√ß√£o): 0.9934187783546217\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Dados artificiais\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=15, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Modelo equivalente ao Ridge com early stopping\n",
    "modelo = SGDRegressor(\n",
    "    penalty='l2',           # regulariza√ß√£o L2 (Ridge)\n",
    "    alpha=0.001,            # for√ßa da regulariza√ß√£o\n",
    "    early_stopping=True,    # ativa parada antecipada\n",
    "    validation_fraction=0.1, # usa 10% dos dados de treino para valida√ß√£o\n",
    "    n_iter_no_change=5,     # para se n√£o houver melhora em 5 √©pocas\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "modelo.fit(X_train, y_train)\n",
    "print(\"Coeficientes:\", modelo.coef_)\n",
    "print(\"R¬≤ (valida√ß√£o):\", modelo.score(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98db1225-35cc-4ec8-8c0f-773d13c535d7",
   "metadata": {},
   "source": [
    "‚öñÔ∏è **Resumo simples**\n",
    "\n",
    "| Conceito           | Explica√ß√£o |\n",
    "|--------------------|------------|\n",
    "| **Ridge (L2)** | Regress√£o Linear com penaliza√ß√£o nos pesos |\n",
    "| **Objetivo** | Reduzir overfitting e melhorar generaliza√ß√£o |\n",
    "| **Par√¢metro principal** | `alpha` ‚Üí controla a for√ßa da penaliza√ß√£o |\n",
    "| **Early Stopping** | Interrompe o treino antes do overfitting (usando `SGDRegressor`) |\n",
    "| **Efeito pr√°tico** | Pesos pequenos, modelo est√°vel e previs√µes mais suaves |\n",
    "\n",
    "ü™Ñ **Em resumo:**  \n",
    "O **Ridge (L2)** √© como um *freio* no modelo linear:  \n",
    "ele impede que os pesos cres√ßam demais e, com **early stopping**, tamb√©m evita que o modelo aprenda al√©m do necess√°rio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcf6923-6d1a-4f8d-b3ca-72db21e8cfc1",
   "metadata": {},
   "source": [
    "## **Lasso Regression (L1 Regularization)**\n",
    "\n",
    "### üí° **O que √© o Lasso Regression (L1 Regularization)**\n",
    "\n",
    "O **Lasso** √© um modelo linear regularizado, assim como o **Ridge**, mas com uma diferen√ßa importante:\n",
    "\n",
    "üëâ Ele usa a **penaliza√ß√£o L1**, que √© baseada no **valor absoluto dos pesos** ‚Äî n√£o no quadrado deles.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **Para que serve**\n",
    "\n",
    "O Lasso serve para:\n",
    "\n",
    "- Evitar **overfitting**, deixando o modelo mais simples.  \n",
    "- **Selecionar vari√°veis automaticamente** (zera pesos de features menos importantes).  \n",
    "- Melhorar a **interpretabilidade** do modelo (mant√©m s√≥ as vari√°veis mais relevantes).\n",
    "\n",
    "üí¨ **Em resumo:**  \n",
    "O **Lasso** √© √≥timo quando voc√™ tem **muitas vari√°veis** e quer descobrir **quais s√£o realmente importantes**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è **Como funciona**\n",
    "\n",
    "Na regress√£o linear tradicional, a fun√ß√£o de custo √©:\n",
    "\n",
    "$$\n",
    "J(w) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y_i})^2\n",
    "$$\n",
    "\n",
    "O **Lasso** adiciona a penaliza√ß√£o L1:\n",
    "\n",
    "$$\n",
    "J(w) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y_i})^2 + \\lambda \\sum_{j=1}^{n} |w_j|\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "üîπ **Onde:**\n",
    "\n",
    "- \\( w_j \\): pesos do modelo  \n",
    "- \\( \\lambda \\): for√ßa da regulariza√ß√£o (em *Scikit-Learn*, √© o par√¢metro `alpha`)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### ‚öôÔ∏è **Par√¢metro principal**\n",
    "\n",
    "| Par√¢metro | Significado |\n",
    "|------------|-------------|\n",
    "| **alpha** | Controla a for√ßa da regulariza√ß√£o (quanto maior, mais forte a penaliza√ß√£o e mais coeficientes zerados) |\n",
    "\n",
    "---\n",
    "\n",
    "### üß† **Intui√ß√£o simples**\n",
    "\n",
    "üëâ Enquanto o **Ridge (L2)** apenas **reduz os pesos**, o **Lasso (L1)** pode **zerar completamente alguns deles**.  \n",
    "\n",
    "üîπ Isso faz com que o **Lasso** escolha automaticamente quais vari√°veis s√£o mais importantes ‚Äî  \n",
    "ele √© como um **‚Äúmodelo linear com poda de vari√°veis‚Äù**.\n",
    "\n",
    "‚û°Ô∏è **Note:** alguns coeficientes podem aparecer como **0** ‚Äî  \n",
    "o **Lasso removeu essas vari√°veis** por considerar que t√™m **pouca import√¢ncia**.\n",
    "\n",
    "üß© **Exemplo pr√°tico**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6439a2be-57e2-4e94-ad1e-0fb3a19daf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes: [61.91052278 98.2580621  59.18981203 54.67148179 36.0723    ]\n",
      "Intercepto: 0.8262520221685765\n",
      "R¬≤ no teste: 0.9875357761517428\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Gerar dados artificiais\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=15, random_state=42)\n",
    "\n",
    "# Dividir treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Modelo Lasso (regulariza√ß√£o L1)\n",
    "modelo = Lasso(alpha=0.1)  # alpha controla a for√ßa da regulariza√ß√£o\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "print(\"Coeficientes:\", modelo.coef_)\n",
    "print(\"Intercepto:\", modelo.intercept_)\n",
    "print(\"R¬≤ no teste:\", modelo.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cb8635-8aa4-420e-bad7-94092ae549a1",
   "metadata": {},
   "source": [
    "## üß© **Parada antecipada (Early Stopping) com Lasso**\n",
    "\n",
    "Assim como no **Ridge**, o modelo **Lasso** padr√£o **n√£o tem early stopping direto**.  \n",
    "Mas √© poss√≠vel usar o **`SGDRegressor`**, configurado com **penaliza√ß√£o L1**, para ativar a **parada antecipada**.\n",
    "\n",
    "---\n",
    "\n",
    "üß† **Exemplo com Early Stopping**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0890e43f-8b36-44df-a26e-5182b33b9b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes: [79.7234915  98.91078512  5.29813461  0.37526144 86.52976826 -0.63794644\n",
      " 69.4800908   0.          0.14621581 -0.16856992 19.14403013 39.84949426\n",
      "  0.15183258  3.00301732 -0.4387512  26.58204968 -0.315611   87.05614332\n",
      "  0.45175848  0.17247852]\n",
      "R¬≤ (valida√ß√£o): 0.9970619061783728\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Dados artificiais\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=10, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Modelo equivalente ao Lasso com early stopping\n",
    "modelo = SGDRegressor(\n",
    "    penalty='l1',            # regulariza√ß√£o L1 (Lasso)\n",
    "    alpha=0.001,             # for√ßa da regulariza√ß√£o\n",
    "    early_stopping=True,     # ativa parada antecipada\n",
    "    validation_fraction=0.1, # usa 10% dos dados de treino para valida√ß√£o\n",
    "    n_iter_no_change=5,      # para se n√£o houver melhora em 5 √©pocas\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "modelo.fit(X_train, y_train)\n",
    "print(\"Coeficientes:\", modelo.coef_)\n",
    "print(\"R¬≤ (valida√ß√£o):\", modelo.score(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada67319-3028-49fb-8f98-29f5e01c56a9",
   "metadata": {},
   "source": [
    "## üìâ **Como funciona a parada antecipada no Lasso**\n",
    "\n",
    "- O modelo √© treinado com **descida de gradiente**.  \n",
    "- A cada √©poca, mede-se o **erro nos dados de valida√ß√£o**.  \n",
    "- Se o erro n√£o melhorar ap√≥s um certo n√∫mero de itera√ß√µes (`n_iter_no_change`), o **treinamento para automaticamente**.  \n",
    "- Assim, o modelo **n√£o continua aprendendo ru√≠dos** e **evita overfitting**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öñÔ∏è **Resumo simples**\n",
    "\n",
    "| Conceito | Explica√ß√£o |\n",
    "|-----------|-------------|\n",
    "| **Lasso (L1)** | Regress√£o Linear com penaliza√ß√£o pelo valor absoluto dos pesos |\n",
    "| **Objetivo** | Reduzir overfitting e fazer sele√ß√£o autom√°tica de vari√°veis |\n",
    "| **Par√¢metro principal** | `alpha` ‚Üí for√ßa da penaliza√ß√£o |\n",
    "| **Early Stopping** | Interrompe o treino antes do overfitting (usando `SGDRegressor`) |\n",
    "| **Efeito pr√°tico** | Alguns pesos s√£o zerados ‚Üí modelo mais simples e interpret√°vel |\n",
    "\n",
    "---\n",
    "\n",
    "ü™Ñ **Em resumo:**  \n",
    "O **Lasso (L1)** √© como um **‚Äúfiltro inteligente‚Äù**:  \n",
    "ele **mant√©m apenas as vari√°veis mais importantes** e **zera o resto**.  \n",
    "Com **early stopping**, ele tamb√©m sabe **quando parar de aprender antes de come√ßar a errar demais**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdc73d2-4dde-40ae-8a6a-3e2730d5ee7f",
   "metadata": {},
   "source": [
    "## **Elastic Net**\n",
    "\n",
    "### üí° **O que √© o Elastic Net**\n",
    "\n",
    "O **Elastic Net** √© um modelo linear regularizado que **combina duas penaliza√ß√µes ao mesmo tempo**:\n",
    "\n",
    "- **L1 (Lasso)** ‚Üí zera alguns pesos (seleciona vari√°veis)  \n",
    "- **L2 (Ridge)** ‚Üí reduz os pesos grandes (evita overfitting)\n",
    "\n",
    "üëâ Assim, ele junta as **vantagens dos dois m√©todos**.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **Para que serve**\n",
    "\n",
    "O Elastic Net serve para:\n",
    "\n",
    "- ‚úÖ Evitar **overfitting** como o Ridge  \n",
    "- ‚úÖ Selecionar **vari√°veis importantes** como o Lasso  \n",
    "- ‚úÖ Lidar bem com **dados correlacionados**, onde o Lasso puro √†s vezes falha  \n",
    "\n",
    "üí¨ **Em resumo:**  \n",
    "O Elastic Net √© √∫til quando voc√™ tem **muitas vari√°veis** e suspeita que **v√°rias delas s√£o correlacionadas entre si**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è **Como funciona**\n",
    "\n",
    "A fun√ß√£o de custo combina as penaliza√ß√µes **L1 e L2**:\n",
    "\n",
    "$$\n",
    "J(w) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 + \\lambda \\left[ (1 - r) \\sum \\frac{w_j^2}{2} + r \\sum |w_j| \\right]\n",
    "$$\n",
    "\n",
    "üîπ **Onde:**\n",
    "\n",
    "- **Œª (lambda):** controla o quanto de regulariza√ß√£o total h√°  \n",
    "- **r (l1_ratio no Scikit-Learn):** controla o equil√≠brio entre L1 e L2  \n",
    "\n",
    "üìò **Casos especiais:**\n",
    "- `l1_ratio = 1` ‚Üí vira **Lasso puro**  \n",
    "- `l1_ratio = 0` ‚Üí vira **Ridge puro**\n",
    "\n",
    "---\n",
    "\n",
    "### üß† **Intui√ß√£o simples**\n",
    "\n",
    "Pense assim üëá\n",
    "\n",
    "| Modelo | Efeito principal |\n",
    "|--------|------------------|\n",
    "| **Ridge (L2)** | Diminui todos os pesos, mas n√£o zera |\n",
    "| **Lasso (L1)** | Zera alguns pesos, mas √†s vezes remove demais |\n",
    "| **Elastic Net** | Mistura os dois: reduz uns, zera outros e mant√©m equil√≠brio |\n",
    "\n",
    "---\n",
    "\n",
    "### üß© **Exemplo pr√°tico**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19afa69f-23b3-4638-bc0c-37c311bc733f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes: [20.54438455 51.7675573   2.80297662 60.06665584 86.88547419 65.48173303\n",
      " 79.43028704  5.93766659  3.21392784 66.51674311]\n",
      "Intercepto: 3.044218596391934\n",
      "R¬≤ no teste: 0.9920398601480124\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Gerar dados artificiais\n",
    "X, y = make_regression(n_samples=100, n_features=10, noise=15, random_state=42)\n",
    "\n",
    "# Dividir treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Modelo Elastic Net\n",
    "modelo = ElasticNet(alpha=0.1, l1_ratio=0.5)  # mistura 50% L1 e 50% L2\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "print(\"Coeficientes:\", modelo.coef_)\n",
    "print(\"Intercepto:\", modelo.intercept_)\n",
    "print(\"R¬≤ no teste:\", modelo.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c855cee-c91a-4c40-af51-099f0532d80f",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è **Par√¢metros principais**\n",
    "\n",
    "| Par√¢metro | Significado |\n",
    "|------------|-------------|\n",
    "| **alpha** | Controla a for√ßa total da regulariza√ß√£o |\n",
    "| **l1_ratio** | Mistura entre L1 e L2 (`0 = s√≥ Ridge`, `1 = s√≥ Lasso`) |\n",
    "\n",
    "---\n",
    "\n",
    "### üß© **Parada antecipada (Early Stopping) com Elastic Net**\n",
    "\n",
    "Assim como nos outros casos, o **ElasticNet** padr√£o n√£o tem *early stopping* direto,  \n",
    "mas podemos usar o **`SGDRegressor`** com `penalty='elasticnet'`, que permite isso! ‚öôÔ∏è\n",
    "\n",
    "---\n",
    "\n",
    "### üß† **Exemplo com Early Stopping**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39202171-18a0-43cb-8c73-eeed69a33f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficientes: [ 7.96859128e+01  9.88581736e+01  5.29524518e+00  3.79419834e-01\n",
      "  8.64866167e+01 -6.38231755e-01  6.94428217e+01  4.88626419e-03\n",
      "  1.44315172e-01 -1.66683481e-01  1.91321531e+01  3.98291305e+01\n",
      "  1.54738360e-01  2.99785860e+00 -4.38079561e-01  2.65721425e+01\n",
      " -3.20901114e-01  8.70106212e+01  4.53933076e-01  1.74918035e-01]\n",
      "R¬≤ (valida√ß√£o): 0.997062474512175\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Dados artificiais\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=10, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Modelo equivalente ao Elastic Net com early stopping\n",
    "modelo = SGDRegressor(\n",
    "    penalty='elasticnet',   # regulariza√ß√£o mista L1 + L2\n",
    "    alpha=0.001,            # for√ßa total da regulariza√ß√£o\n",
    "    l1_ratio=0.5,           # 50% L1 e 50% L2\n",
    "    early_stopping=True,    # ativa parada antecipada\n",
    "    validation_fraction=0.1,# usa 10% do treino para valida√ß√£o\n",
    "    n_iter_no_change=5,     # para se n√£o houver melhora em 5 √©pocas\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "modelo.fit(X_train, y_train)\n",
    "print(\"Coeficientes:\", modelo.coef_)\n",
    "print(\"R¬≤ (valida√ß√£o):\", modelo.score(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe3f19f-c036-492d-9175-278541dc8403",
   "metadata": {},
   "source": [
    "### üìâ **Como funciona a parada antecipada no Elastic Net**\n",
    "\n",
    "O modelo √© treinado com **descida de gradiente**.  \n",
    "\n",
    "A cada itera√ß√£o, o **erro √© medido no conjunto de valida√ß√£o**.  \n",
    "\n",
    "Se o erro **n√£o melhorar ap√≥s algumas √©pocas**, o treino √© **interrompido automaticamente**.  \n",
    "\n",
    "Assim, ele **n√£o aprende demais** e **evita overfitting**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öñÔ∏è **Resumo simples**\n",
    "\n",
    "| Conceito | Explica√ß√£o |\n",
    "|-----------|-------------|\n",
    "| **Elastic Net** | Combina as penaliza√ß√µes do Lasso (L1) e do Ridge (L2) |\n",
    "| **Objetivo** | Reduz overfitting e seleciona vari√°veis de forma equilibrada |\n",
    "| **Par√¢metros principais** | `alpha` (for√ßa total), `l1_ratio` (equil√≠brio L1/L2) |\n",
    "| **Early Stopping** | Interrompe o treino antes do overfitting (usando `SGDRegressor`) |\n",
    "| **Efeito pr√°tico** | Alguns pesos s√£o reduzidos, outros zerados ‚Üí modelo mais est√°vel e simples |\n",
    "\n",
    "---\n",
    "\n",
    "ü™Ñ **Em resumo:**  \n",
    "O **Elastic Net** √© o *‚Äúmeio-termo inteligente‚Äù* entre o **Ridge** e o **Lasso**.  \n",
    "Ele **reduz pesos grandes**, mas **sem zerar tudo**, e com *early stopping*, ainda **sabe o momento certo de parar**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0e92fe-7dab-4325-9f6e-dc2cc928e722",
   "metadata": {},
   "source": [
    "## **Parametros das Func√µes**\n",
    "\n",
    "| Par√¢metro       | Fun√ß√£o                              | Tipo                  | Valores poss√≠veis / padr√£o                                                      | Ridge | Lasso | Elastic Net | Observa√ß√µes                                                                 |\n",
    "|-----------------|------------------------------------|---------------------|-------------------------------------------------------------------------------|-------|-------|-------------|----------------------------------------------------------------------------|\n",
    "| **alpha**       | For√ßa da regulariza√ß√£o              | float ‚â• 0           | Padr√£o: 1.0 (Ridge), 1.0 (Lasso), 1.0 (Elastic Net)                           | ‚úÖ    | ‚úÖ    | ‚úÖ          | Quanto maior, mais forte a regulariza√ß√£o; 0 = sem regulariza√ß√£o            |\n",
    "| **l1_ratio**    | Mistura entre L1 e L2               | float [0,1]         | Padr√£o: 0.5                                                                    | ‚ùå    | ‚ùå    | ‚úÖ          | 0 = s√≥ L2 (Ridge), 1 = s√≥ L1 (Lasso)                                       |\n",
    "| **fit_intercept** | Adiciona termo independente (b)    | bool                | Padr√£o: True                                                                    | ‚úÖ    | ‚úÖ    | ‚úÖ          | Se False, assume que os dados j√° est√£o centralizados                        |\n",
    "| **max_iter**    | N√∫mero m√°ximo de itera√ß√µes          | int ‚â• 1             | Padr√£o: 1000                                                                   | ‚úÖ    | ‚úÖ    | ‚úÖ          | Relevante para solvers iterativos (saga, sparse_cg)                        |\n",
    "| **tol**         | Toler√¢ncia de converg√™ncia          | float > 0           | Padr√£o: 1e-4 ou 1e-3                                                          | ‚úÖ    | ‚úÖ    | ‚úÖ          | Menor = mais preciso, mas mais lento                                        |\n",
    "| **selection**   | Ordem de atualiza√ß√£o dos coeficientes | 'cyclic' / 'random' | Padr√£o: 'cyclic'                                                              | ‚ùå    | ‚úÖ    | ‚úÖ          | Exclusivo para Lasso e Elastic Net                                         |\n",
    "| **solver**      | M√©todo de otimiza√ß√£o                | str                 | 'auto', 'svd', 'cholesky', 'sparse_cg', 'saga', etc.                           | ‚úÖ    | ‚ö†Ô∏è    | ‚ö†Ô∏è          | Ridge tem mais op√ß√µes; Lasso/Elastic Net normalmente usam 'saga' ou m√©todos iterativos |\n",
    "| **positive**    | Restringe coeficientes ‚â• 0          | bool                | False (padr√£o)                                                                 | ‚úÖ    | ‚úÖ    | ‚úÖ          | Pode ser √∫til para regress√£o n√£o negativa                                   |\n",
    "| **random_state** | Semente para aleatoriedade          | int                 | None (padr√£o)                                                                  | ‚úÖ    | ‚úÖ    | ‚úÖ          | Afeta solvers aleat√≥rios, como 'saga' ou coordena√ß√£o aleat√≥ria em selection='random' |\n",
    "| **warm_start**  | Reaproveita solu√ß√£o anterior         | bool                | False                                                                          | ‚úÖ    | ‚úÖ    | ‚úÖ          | √ötil para ajustes incrementais ou tuning de alpha                           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc44550-8442-4d9d-bc1e-2667e0d843b0",
   "metadata": {},
   "source": [
    "üîπ **Observa√ß√µes importantes**\n",
    "\n",
    "**Ridge**\n",
    "- Regulariza√ß√£o L2 apenas.\n",
    "- Tem `solver` e `positive`.\n",
    "- N√£o tem `selection` nem `l1_ratio`.\n",
    "\n",
    "**Lasso**\n",
    "- Regulariza√ß√£o L1, alguns coeficientes podem ser zerados.\n",
    "- Tem `selection` para coordenadas c√≠clicas ou aleat√≥rias.\n",
    "- Solvers recomendados: `saga` ou m√©todos iterativos.\n",
    "\n",
    "**Elastic Net**\n",
    "- Combina L1 e L2.\n",
    "- Tem `l1_ratio` para definir equil√≠brio L1/L2.\n",
    "- Tamb√©m tem `selection` como Lasso.\n",
    "\n",
    "**Par√¢metros comuns**\n",
    "- `alpha`, `fit_intercept`, `max_iter`, `tol`, `random_state`, `warm_start` s√£o comuns aos tr√™s modelos.\n",
    "\n",
    "**Par√¢metros exclusivos**\n",
    "- `l1_ratio` ‚Üí s√≥ Elastic Net.\n",
    "- `selection` ‚Üí Lasso e Elastic Net.\n",
    "- `solver` ‚Üí Ridge tem mais op√ß√µes que Lasso/Elastic Net.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b298ee27-ccf5-42bd-aceb-15421414dd61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
