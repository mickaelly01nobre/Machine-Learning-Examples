{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6682ef86-8ce6-4519-9027-9591876b18c7",
   "metadata": {},
   "source": [
    "# Ãrvore de DecisÃ£o - RegressÃ£o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57590c6b-8beb-4cd2-b3e3-23070b12c69a",
   "metadata": {},
   "source": [
    "# Exemplo de RegressÃ£o com Ãrvore de DecisÃ£o\n",
    "\n",
    "Imagine que vocÃª quer prever a **nota de um aluno** baseado nas horas de estudo:\n",
    "\n",
    "| Horas de estudo | Nota |\n",
    "|-----------------|------|\n",
    "| 2               | 20   |\n",
    "| 4               | 25   |\n",
    "| 5               | 30   |\n",
    "| 7               | 35   |\n",
    "| 9               | 40   |\n",
    "\n",
    "---\n",
    "\n",
    "## Passo 1: ComeÃ§amos com todos os dados juntos\n",
    "\n",
    "Inicialmente, todos os dados estÃ£o no **nÃ³ raiz**.  \n",
    "Objetivo: dividir os dados em grupos que sejam mais **iguais possÃ­veis** em termos de nota.\n",
    "\n",
    "---\n",
    "\n",
    "## Passo 2: Escolher a melhor divisÃ£o\n",
    "\n",
    "A Ã¡rvore procura um ponto de corte nas **horas de estudo** que minimize a diferenÃ§a dentro de cada grupo.\n",
    "\n",
    "Exemplo: Horas > 5?\n",
    "\n",
    "- **Grupo 1 (Horas â‰¤ 5):** notas 20, 25, 30 â†’ mÃ©dia = 25  \n",
    "- **Grupo 2 (Horas > 5):** notas 35, 40 â†’ mÃ©dia = 37,5  \n",
    "\n",
    "Essa divisÃ£o reduz a **variaÃ§Ã£o dentro de cada grupo**.\n",
    "\n",
    "---\n",
    "\n",
    "## Passo 3: Repetir para cada grupo\n",
    "\n",
    "Agora cada grupo pode ser dividido novamente:\n",
    "\n",
    "- **Grupo 1 (Horas â‰¤ 5) â†’ dividir em Horas > 3?**  \n",
    "  - Grupo 1a: Horas â‰¤ 3 â†’ nota 20 â†’ mÃ©dia = 20  \n",
    "  - Grupo 1b: Horas > 3 â†’ notas 25, 30 â†’ mÃ©dia = 27,5\n",
    "\n",
    "- **Grupo 2 (Horas > 5) â†’ dividir em Horas > 8?**  \n",
    "  - Grupo 2a: Horas â‰¤ 8 â†’ nota 35 â†’ mÃ©dia = 35  \n",
    "  - Grupo 2b: Horas > 8 â†’ nota 40 â†’ mÃ©dia = 40\n",
    "\n",
    "---\n",
    "\n",
    "## Passo 4: Parar quando nÃ£o faz sentido dividir mais\n",
    "\n",
    "**CritÃ©rios para parar a divisÃ£o:**\n",
    "\n",
    "- NÃºmero mÃ­nimo de dados no grupo (ex: pelo menos 2)  \n",
    "- A variaÃ§Ã£o dentro do grupo jÃ¡ Ã© pequena  \n",
    "- Profundidade mÃ¡xima da Ã¡rvore foi atingida\n",
    "\n",
    "Cada grupo final Ã© chamado de **folha**.  \n",
    "O valor da folha Ã© a **mÃ©dia das notas do grupo**.\n",
    "\n",
    "---\n",
    "\n",
    "## Passo 5: Fazer previsÃ£o\n",
    "\n",
    "Para prever a nota de alguÃ©m que estudou 6 horas:\n",
    "\n",
    "1. Horas > 5? â†’ **Sim** â†’ Grupo 2  \n",
    "2. Horas > 8? â†’ **NÃ£o** â†’ Grupo 2a  \n",
    "\n",
    "**PrevisÃ£o = mÃ©dia do Grupo 2a = 35**\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Resumo simples\n",
    "\n",
    "- ComeÃ§a com todos os dados  \n",
    "- Escolhe a melhor divisÃ£o que deixa os grupos **mais homogÃªneos**  \n",
    "- Repete o processo para cada grupo  \n",
    "- Para quando os grupos estiverem pequenos ou homogÃªneos  \n",
    "- **Valor previsto = mÃ©dia dos valores da folha correspondente**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25151824-5e8e-40f5-a3cb-34d7fd9f018f",
   "metadata": {},
   "source": [
    "# ğŸ¯ 1. MSE â€“ Mean Squared Error (Erro QuadrÃ¡tico MÃ©dio)\n",
    "\n",
    "ğŸ“˜ **FÃ³rmula:**\n",
    "\n",
    "\\[\n",
    "MSE = \\frac{1}{n} \\sum (y_{real} - y_{previsto})^2\n",
    "\\]\n",
    "\n",
    "ğŸ‘‰ **O que significa:**  \n",
    "Ã‰ a **mÃ©dia dos quadrados dos erros** entre o valor real e o valor previsto.  \n",
    "Como o erro Ã© elevado ao quadrado, **erros grandes** pesam muito mais.\n",
    "\n",
    "ğŸ”¹ **Serve para:** medir o quÃ£o longe, em mÃ©dia, o modelo estÃ¡ dos valores corretos.  \n",
    "ğŸ”¹ **Quanto menor, melhor.**  \n",
    "ğŸ”¹ Muito usada durante o **treinamento do modelo**.\n",
    "\n",
    "ğŸ“Š **Exemplo:**  \n",
    "Se o modelo erra pouco (tipo 0.1 ou 0.2), o MSE serÃ¡ pequeno (ex: 0.04).  \n",
    "Mas se ele errar feio (tipo 2 ou 3), o MSE cresce rÃ¡pido (ex: 9).\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ’¡ 2. RMSE â€“ Root Mean Squared Error (Raiz do Erro QuadrÃ¡tico MÃ©dio)\n",
    "\n",
    "ğŸ“˜ **FÃ³rmula:**\n",
    "\n",
    "\\[\n",
    "RMSE = \\sqrt{MSE}\n",
    "\\]\n",
    "\n",
    "ğŸ‘‰ **O que significa:**  \n",
    "Ã‰ apenas a **raiz quadrada do MSE**.  \n",
    "Ela traz o erro de volta para a **mesma unidade da variÃ¡vel y**.\n",
    "\n",
    "ğŸ”¹ **Serve para:** entender o erro de forma mais intuitiva (na mesma â€œescalaâ€ dos dados reais).  \n",
    "ğŸ”¹ **Quanto menor, melhor.**\n",
    "\n",
    "ğŸ“Š **Exemplo:**  \n",
    "Se vocÃª estÃ¡ prevendo o preÃ§o de uma casa em reais e o RMSE deu **50.000**, isso quer dizer que, em mÃ©dia, o modelo erra cerca de **R$ 50 mil por previsÃ£o**.\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ“ 3. MAE â€“ Mean Absolute Error (Erro Absoluto MÃ©dio)\n",
    "\n",
    "ğŸ“˜ **FÃ³rmula:**\n",
    "\n",
    "\\[\n",
    "MAE = \\frac{1}{n} \\sum |y_{real} - y_{previsto}|\n",
    "\\]\n",
    "\n",
    "ğŸ‘‰ **O que significa:**  \n",
    "Ã‰ a **mÃ©dia do valor absoluto dos erros** â€” ou seja, quanto o modelo erra em mÃ©dia, **sem elevar ao quadrado**.\n",
    "\n",
    "ğŸ”¹ **Serve para:** medir o erro de forma simples e robusta (sem dar tanto peso a erros muito grandes).  \n",
    "ğŸ”¹ **Quanto menor, melhor.**\n",
    "\n",
    "ğŸ“Š **Exemplo:**  \n",
    "Se o MAE = 2.0, significa que o modelo, em mÃ©dia, erra **2 unidades** (pode ser 2Â°C, 2 km, 2 reais etc).\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ“ˆ 4. RÂ² â€“ Coeficiente de DeterminaÃ§Ã£o\n",
    "\n",
    "ğŸ“˜ **FÃ³rmula (simplificada):**\n",
    "\n",
    "\\[\n",
    "R^2 = 1 - \\frac{\\text{soma dos erros do modelo}}{\\text{soma dos erros de um modelo â€œburroâ€ (mÃ©dia)}}\n",
    "\\]\n",
    "\n",
    "ğŸ‘‰ **O que significa:**  \n",
    "Mostra **quanto da variaÃ§Ã£o total dos dados o modelo consegue explicar**.\n",
    "\n",
    "ğŸ”¹ **Valores:**\n",
    "- RÂ² = 1 â†’ o modelo Ã© perfeito  \n",
    "- RÂ² = 0 â†’ o modelo nÃ£o explica nada  \n",
    "- RÂ² < 0 â†’ o modelo Ã© pior que adivinhar pela mÃ©dia  \n",
    "\n",
    "ğŸ“Š **Exemplo:**  \n",
    "Se RÂ² = 0.9, quer dizer que **90% da variaÃ§Ã£o dos dados reais Ã© explicada pelo modelo** â€” Ã³timo!\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ“‹ Resumo rÃ¡pido\n",
    "\n",
    "| MÃ©trica | Nome | InterpretaÃ§Ã£o | Melhor valor |\n",
    "|----------|------|----------------|---------------|\n",
    "| **MSE** | Erro QuadrÃ¡tico MÃ©dio | Penaliza erros grandes | 0 |\n",
    "| **RMSE** | Raiz do Erro QuadrÃ¡tico MÃ©dio | Mesmo significado do MSE, mas na escala original dos dados | 0 |\n",
    "| **MAE** | Erro Absoluto MÃ©dio | MÃ©dia simples dos erros | 0 |\n",
    "| **RÂ²** | Coeficiente de DeterminaÃ§Ã£o | Mede quanto o modelo explica a variaÃ§Ã£o dos dados | 1 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16c6513-c57b-402e-acfc-50ea0a2942cf",
   "metadata": {},
   "source": [
    "## âš™ï¸ Exemplo de CÃ³digo\n",
    "\n",
    "### ğŸ§  ExplicaÃ§Ã£o simples do que cada mÃ©trica faz\n",
    "\n",
    "- **`DecisionTreeRegressor(max_depth=3)`** â†’ cria uma **Ã¡rvore de regressÃ£o** com profundidade mÃ¡xima igual a **3**.  \n",
    "- **`fit(X_train, y_train)`** â†’ **treina** a Ã¡rvore com os dados de treino.  \n",
    "- **`predict(X_test)`** â†’ faz **previsÃµes** para os dados de teste.  \n",
    "- **`mean_squared_error`** â†’ calcula o **erro quadrÃ¡tico mÃ©dio (MSE)** â€” mostra o quanto os valores previstos estÃ£o longe dos reais.  \n",
    "- **`np.sqrt(mse)`** â†’ calcula o **RMSE**, trazendo o erro para a **mesma escala da variÃ¡vel y**.  \n",
    "- **`mean_absolute_error`** â†’ calcula o **erro absoluto mÃ©dio (MAE)** â€” mostra o erro mÃ©dio sem elevar ao quadrado.  \n",
    "- **`r2_score`** â†’ mede **quanto da variaÃ§Ã£o dos dados o modelo explica** (*1 = perfeito*, *0 = nÃ£o explica nada*).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f350cc2-7a53-4a90-ac19-6afd3bdb08f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PrevisÃµes: [7.5 1.5 5. ]\n",
      "Valores reais: [8.  3.2 5.5]\n",
      "MSE: 1.130\n",
      "RMSE: 1.063\n",
      "MAE: 0.900\n",
      "RÂ²: 0.706\n"
     ]
    }
   ],
   "source": [
    "# Importando bibliotecas\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Criando um conjunto de dados de exemplo\n",
    "# Vamos prever \"y\" baseado em \"X\"\n",
    "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]])\n",
    "y = np.array([1.5, 3.2, 2.8, 4.5, 5.0, 5.5, 7.2, 7.5, 8.0, 9.1])\n",
    "\n",
    "# Dividindo em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Criando a Ã¡rvore de regressÃ£o\n",
    "tree_reg = DecisionTreeRegressor(max_depth=3, random_state=42)\n",
    "tree_reg.fit(X_train, y_train)  # Treinando o modelo\n",
    "\n",
    "# Fazendo previsÃµes no conjunto de teste\n",
    "y_pred = tree_reg.predict(X_test)\n",
    "\n",
    "# Calculando mÃ©tricas de regressÃ£o\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Mostrando resultados\n",
    "print(\"PrevisÃµes:\", y_pred)\n",
    "print(\"Valores reais:\", y_test)\n",
    "print(f\"MSE: {mse:.3f}\")\n",
    "print(f\"RMSE: {rmse:.3f}\")\n",
    "print(f\"MAE: {mae:.3f}\")\n",
    "print(f\"RÂ²: {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7257653-8fb9-4cf4-a9e3-3cfc930bd1ca",
   "metadata": {},
   "source": [
    "## âš™ï¸ ExplicaÃ§Ã£o do Codigo de Ãrvore de DecisÃ£o (RegressÃ£o) do Git do Colab Notebook \"Hands-on Machine Learning, 3rd edition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0348ea-deaa-44e1-b828-2c385288c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O que faz:\n",
    "# Cria um conjunto de pontos x1 uniformemente espaÃ§ados dentro do intervalo definido em axes.\n",
    "# Usa a Ã¡rvore de regressÃ£o (tree_reg) para prever os valores y_pred para esses pontos.\n",
    "# Configura os eixos do grÃ¡fico.\n",
    "# Plota os pontos reais (X, y) em azul.\n",
    "# Plota a linha de previsÃ£o da Ã¡rvore (y_pred) em vermelho.\n",
    "# Resumo: essa funÃ§Ã£o desenha como a Ã¡rvore de regressÃ£o â€œestimaâ€ o valor de y ao longo do eixo x.\n",
    "\n",
    "# Define uma funÃ§Ã£o chamada plot_regression_predictions.\n",
    "# ParÃ¢metros:\n",
    "# tree_reg â†’ Ã¡rvore de regressÃ£o que jÃ¡ foi treinada.\n",
    "# X â†’ dados de entrada (features).\n",
    "# y â†’ valores reais (alvo).\n",
    "# axes â†’ limites do grÃ¡fico [xmin, xmax, ymin, ymax]\n",
    "\n",
    "def plot_regression_predictions(tree_reg, X, y, axes=[-0.5, 0.5, -0.05, 0.25]):\n",
    "    # Cria 500 pontos igualmente espaÃ§ados entre axes[0] e axes[1].\n",
    "    # reshape(-1, 1) transforma o array em coluna, como o modelo espera.\n",
    "    x1 = np.linspace(axes[0], axes[1], 500).reshape(-1, 1)\n",
    "    \n",
    "    # Usa a Ã¡rvore de regressÃ£o para prever os valores de y para os pontos x1.\n",
    "    # y_pred serÃ¡ a linha da Ã¡rvore que mostra como ela prevÃª os valores.\n",
    "    y_pred = tree_reg.predict(x1)\n",
    "\n",
    "    # Define os limites do grÃ¡fico conforme axes (xmin, xmax, ymin, ymax).\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(\"$x_1$\")\n",
    "\n",
    "    # Plota os pontos reais (X vs y) em azul (b. significa blue dots).\n",
    "    plt.plot(X, y, \"b.\")\n",
    "\n",
    "    # Plota a previsÃ£o da Ã¡rvore (y_pred) em vermelho com pontos e linha contÃ­nua (r.-).\n",
    "    # linewidth=2 â†’ espessura da linha.\n",
    "    # label=\"$\\hat{y}$\" â†’ adiciona legenda para a linha prevista.\n",
    "    plt.plot(x1, y_pred, \"r.-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
    "\n",
    "# O que faz:\n",
    "# Cria dois grÃ¡ficos lado a lado (ncols=2).\n",
    "# Seleciona o primeiro grÃ¡fico (axes[0]) e plota a previsÃ£o da Ã¡rvore (tree_reg) usando \n",
    "# os dados X_quad e y_quad.\n",
    "\n",
    "# Cria dois grÃ¡ficos lado a lado (ncols=2).\n",
    "# figsize=(10,4) â†’ tamanho da figura.\n",
    "# sharey=True â†’ compartilha o eixo y entre os dois grÃ¡ficos.\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "\n",
    "#Seleciona o primeiro grÃ¡fico (Ã  esquerda) para plotar.\n",
    "plt.sca(axes[0])\n",
    "\n",
    "#Chama a funÃ§Ã£o que plota a previsÃ£o da Ã¡rvore e os dados reais para o primeiro subplot.\n",
    "plot_regression_predictions(tree_reg, X_quad, y_quad)\n",
    "\n",
    "# O que faz:\n",
    "# Pega os valores de corte (thresholds) que a Ã¡rvore usa para dividir os dados.\n",
    "# Desenha linhas verticais nesses cortes para mostrar visualmente onde a Ã¡rvore divide os dados.\n",
    "# k- â†’ linha preta sÃ³lida (profundidade 0, raiz)\n",
    "# k-- â†’ linha preta tracejada (profundidade 1)\n",
    "# Adiciona textos para mostrar a profundidade do nÃ³ na Ã¡rvore.\n",
    "# Resumo: vocÃª consegue ver onde a Ã¡rvore corta os dados para fazer suas previsÃµes.\n",
    "\n",
    "# Pega os valores de corte (thresholds) da Ã¡rvore nos nÃ³s 0, 1 e 4.\n",
    "# Esses thresholds sÃ£o os pontos onde a Ã¡rvore divide os dados.\n",
    "th0, th1a, th1b = tree_reg.tree_.threshold[[0, 1, 4]]\n",
    "\n",
    "# Desenha linhas verticais nos thresholds.\n",
    "# k- â†’ linha preta sÃ³lida (profundidade 0), k-- â†’ linha preta tracejada (profundidade 1).\n",
    "# Mostra visualmente onde a Ã¡rvore divide os dados.\n",
    "for split, style in ((th0, \"k-\"), (th1a, \"k--\"), (th1b, \"k--\")):\n",
    "    plt.plot([split, split], [-0.05, 0.25], style, linewidth=2)\n",
    "\n",
    "# Adiciona texto indicando a profundidade do nÃ³ para cada threshold.\n",
    "# Facilita entender qual nÃ³ Ã© raiz e quais sÃ£o filhos.\n",
    "plt.text(th0, 0.16, \"Depth=0\", fontsize=15)\n",
    "plt.text(th1a + 0.01, -0.01, \"Depth=1\", horizontalalignment=\"center\", fontsize=13)\n",
    "plt.text(th1b + 0.01, -0.01, \"Depth=1\", fontsize=13)\n",
    "\n",
    "# plt.ylabel â†’ rÃ³tulo do eixo y.\n",
    "# plt.legend â†’ adiciona legenda mostrando $\\hat{y}$.\n",
    "# plt.title â†’ tÃ­tulo do grÃ¡fico (â€œmax_depth=2â€).\n",
    "plt.ylabel(\"$y$\", rotation=0)\n",
    "plt.legend(loc=\"upper center\", fontsize=16)\n",
    "plt.title(\"max_depth=2\")\n",
    "\n",
    "\n",
    "# Seleciona o segundo subplot.\n",
    "# Pega thresholds da segunda Ã¡rvore (tree_reg2) que tem mais profundidade.\n",
    "# Plota a previsÃ£o da Ã¡rvore e os dados reais.\n",
    "plt.sca(axes[1])\n",
    "th2s = tree_reg2.tree_.threshold[[2, 5, 9, 12]]\n",
    "plot_regression_predictions(tree_reg2, X_quad, y_quad)\n",
    "\n",
    "# Plota novamente as linhas do primeiro grÃ¡fico para referÃªncia.\n",
    "for split, style in ((th0, \"k-\"), (th1a, \"k--\"), (th1b, \"k--\")):\n",
    "    plt.plot([split, split], [-0.05, 0.25], style, linewidth=2)\n",
    "\n",
    "\n",
    "# Plota novas divisÃµes da Ã¡rvore mais profunda usando linha pontilhada fina (k:).\n",
    "for split in th2s:\n",
    "    plt.plot([split, split], [-0.05, 0.25], \"k:\", linewidth=1)\n",
    "\n",
    "# Adiciona texto indicando profundidade e tÃ­tulo do grÃ¡fico (â€œmax_depth=3â€).\n",
    "plt.text(th2s[2] + 0.01, 0.15, \"Depth=2\", fontsize=13)\n",
    "plt.title(\"max_depth=3\")\n",
    "\n",
    "save_fig(\"tree_regression_plot\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
